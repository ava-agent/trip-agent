/**
 * GLM-4.7 API Service for Trip Agent
 * æ™ºè°± AI GLM-4.7 API é›†æˆæœåŠ¡
 * API æ–‡æ¡£: https://open.bigmodel.cn/dev/api
 */

// ============================================================================
// Types and Interfaces
// ============================================================================

export interface GLMMessage {
  role: "system" | "user" | "assistant"
  content: string
}

export interface GLMStreamChunk {
  content: string
  done: boolean
}

export interface GLMConfig {
  apiKey: string
  model?: string
  baseURL?: string
  maxTokens?: number
  temperature?: number
  topP?: number
}

export interface GLMError {
  message: string
  code?: string
  retryable: boolean
}

interface GLMStreamChoice {
  index: number
  delta: {
    role?: string
    content?: string
  }
  finish_reason: string | null
}

interface GLMStreamResponse {
  id: string
  created: number
  model: string
  choices: GLMStreamChoice[]
}

// ============================================================================
// GLM Service Implementation
// ============================================================================

export class GLMService {
  private static config: GLMConfig | null = null
  private static readonly DEFAULT_BASE_URL = "https://open.bigmodel.cn/api/paas/v4"
  private static readonly DEFAULT_MODEL = "glm-4-flash"
  private static readonly DEFAULT_MAX_TOKENS = 4000
  private static readonly DEFAULT_TEMPERATURE = 0.7
  private static readonly DEFAULT_TOP_P = 0.9
  private static readonly MAX_RETRIES = 3
  private static readonly BASE_RETRY_DELAY = 1000

  /**
   * Initialize the GLM service with configuration
   */
  static initialize(config: GLMConfig): void {
    this.config = {
      ...config,
      model: config.model || this.DEFAULT_MODEL,
      baseURL: config.baseURL || this.DEFAULT_BASE_URL,
      maxTokens: config.maxTokens ?? this.DEFAULT_MAX_TOKENS,
      temperature: config.temperature ?? this.DEFAULT_TEMPERATURE,
      topP: config.topP ?? this.DEFAULT_TOP_P,
    }
  }

  /**
   * Check if the service is configured
   */
  static isConfigured(): boolean {
    return this.config !== null && this.config.apiKey.length > 0
  }

  /**
   * Get current configuration
   */
  static getConfig(): GLMConfig | null {
    return this.config
  }

  /**
   * Stream chat completion with retry logic
   */
  static async *streamChat(
    messages: GLMMessage[],
    onChunk?: (chunk: string) => void
  ): AsyncGenerator<GLMStreamChunk, void, unknown> {
    if (!this.isConfigured()) {
      throw new GLMAPIError("GLM service not configured. Please set API key.", "not_configured", false)
    }

    const config = this.config!

    for (let attempt = 0; attempt < this.MAX_RETRIES; attempt++) {
      try {
        yield* this.streamGLM(messages, config, onChunk)
        return
      } catch (error) {
        const glmError = this.handleError(error)

        if (!glmError.retryable || attempt === this.MAX_RETRIES - 1) {
          throw new GLMAPIError(glmError.message, glmError.code, false)
        }

        // Exponential backoff with jitter
        const delay = this.BASE_RETRY_DELAY * Math.pow(2, attempt) + Math.random() * 500
        await this.sleep(delay)
      }
    }
  }

  /**
   * Stream completion from GLM API
   */
  private static async *streamGLM(
    messages: GLMMessage[],
    config: GLMConfig,
    onChunk?: (chunk: string) => void
  ): AsyncGenerator<GLMStreamChunk> {
    const baseURL = config.baseURL!
    const endpoint = `${baseURL}/chat/completions`

    const response = await fetch(endpoint, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${config.apiKey}`,
      },
      body: JSON.stringify({
        model: config.model,
        messages,
        max_tokens: config.maxTokens,
        temperature: config.temperature,
        top_p: config.topP,
        stream: true,
      }),
    })

    if (!response.ok) {
      const errorData = await this.parseErrorResponse(response)
      throw new Error(errorData.error?.message || `HTTP ${response.status}: ${response.statusText}`)
    }

    const reader = response.body?.getReader()
    if (!reader) {
      throw new Error("No response body")
    }

    const decoder = new TextDecoder("utf-8")
    let buffer = ""

    try {
      while (true) {
        const { done, value } = await reader.read()
        if (done) break

        buffer += decoder.decode(value, { stream: true })
        const lines = buffer.split("\n")
        buffer = lines.pop() || ""

        for (const line of lines) {
          const trimmed = line.trim()
          if (!trimmed || trimmed === "data: [DONE]") continue
          if (!trimmed.startsWith("data: ")) continue

          try {
            const jsonStr = trimmed.slice(6)
            const data = JSON.parse(jsonStr) as GLMStreamResponse

            if (data.choices && data.choices[0]) {
              const choice = data.choices[0]
              const content = choice.delta?.content

              if (content) {
                onChunk?.(content)
                yield { content, done: false }
              }

              // Check if stream is complete
              if (choice.finish_reason === "stop" || choice.finish_reason === "length") {
                yield { content: "", done: true }
                return
              }
            }
          } catch (parseError) {
            // Skip invalid JSON lines
            console.debug("Failed to parse SSE chunk:", parseError)
          }
        }
      }
    } finally {
      reader.releaseLock()
    }

    yield { content: "", done: true }
  }

  /**
   * Non-streaming chat completion (for convenience)
   */
  static async chatCompletion(messages: GLMMessage[]): Promise<string> {
    let fullContent = ""

    for await (const chunk of this.streamChat(messages)) {
      if (!chunk.done) {
        fullContent += chunk.content
      }
    }

    return fullContent
  }

  /**
   * Parse error response from GLM API
   */
  private static async parseErrorResponse(response: Response): Promise<{ error: { message: string; code?: string } }> {
    try {
      return await response.json()
    } catch {
      return { error: { message: response.statusText } }
    }
  }

  /**
   * Handle and classify errors
   */
  private static handleError(error: unknown): GLMError {
    if (error instanceof GLMAPIError) {
      return error
    }

    const message = error instanceof Error ? error.message : "Unknown error"
    const errorStr = message.toLowerCase()

    // Rate limit errors (GLM uses 429 for rate limiting)
    if (errorStr.includes("rate limit") || errorStr.includes("429") || errorStr.includes("too many requests")) {
      return {
        message: "API rate limit exceeded. Please try again later.",
        code: "rate_limit",
        retryable: true,
      }
    }

    // Network errors
    if (
      errorStr.includes("network") ||
      errorStr.includes("fetch") ||
      errorStr.includes("connection") ||
      errorStr.includes("timeout")
    ) {
      return {
        message: "Network error. Please check your connection.",
        code: "network",
        retryable: true,
      }
    }

    // Authentication errors
    if (
      errorStr.includes("unauthorized") ||
      errorStr.includes("401") ||
      errorStr.includes("invalid api key") ||
      errorStr.includes("authentication")
    ) {
      return {
        message: "Invalid GLM API key. Please check your configuration.",
        code: "auth",
        retryable: false,
      }
    }

    // Server errors
    if (errorStr.includes("500") || errorStr.includes("502") || errorStr.includes("503")) {
      return {
        message: "GLM server error. Please try again later.",
        code: "server",
        retryable: true,
      }
    }

    // Context length errors
    if (
      errorStr.includes("context") &&
      (errorStr.includes("exceed") || errorStr.includes("too long") || errorStr.includes("max tokens"))
    ) {
      return {
        message: "Request too large. Please reduce the input length.",
        code: "context_length",
        retryable: false,
      }
    }

    // Quota exceeded
    if (errorStr.includes("quota") || errorStr.includes("insufficient")) {
      return {
        message: "API quota exceeded. Please check your GLM account.",
        code: "quota",
        retryable: false,
      }
    }

    // Model not found
    if (errorStr.includes("model") && (errorStr.includes("not found") || errorStr.includes("invalid"))) {
      return {
        message: "Invalid model. Please check the model name.",
        code: "invalid_model",
        retryable: false,
      }
    }

    // Default error
    return {
      message: `GLM API error: ${message}`,
      code: "unknown",
      retryable: false,
    }
  }

  /**
   * Sleep utility for retry delays
   */
  private static sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms))
  }

  /**
   * Reset configuration (useful for testing)
   */
  static reset(): void {
    this.config = null
  }

  /**
   * Get available GLM models
   */
  static getAvailableModels(): string[] {
    return [
      "glm-4-flash", // å¿«é€Ÿå“åº”ï¼Œé€‚åˆå®æ—¶å¯¹è¯
      "glm-4-plus", // é«˜çº§æ¨ç†èƒ½åŠ›
      "glm-4-air", // è½»é‡çº§æ¨¡å‹
      "glm-4", // æ ‡å‡†ç‰ˆæœ¬
      "glm-3-turbo", // ä¸Šä¸€ä»£æ¨¡å‹
    ]
  }
}

// ============================================================================
// Custom Error Class
// ============================================================================

export class GLMAPIError extends Error {
  code?: string
  retryable: boolean

  constructor(message: string, code?: string, retryable: boolean = false) {
    super(message)
    this.name = "GLMAPIError"
    this.code = code
    this.retryable = retryable
  }

  toGLMError(): GLMError {
    return {
      message: this.message,
      code: this.code,
      retryable: this.retryable,
    }
  }
}

// ============================================================================
// Prompt Templates for Trip Planning (GLM Optimized)
// ============================================================================

export const GLM_PROMPTS = {
  /**
   * System prompt for the Supervisor agent
   */
  SUPERVISOR: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡Œè§„åˆ’åŠ©æ‰‹çš„ä¸»ç®¡ Agentã€‚ä½ çš„èŒè´£æ˜¯ï¼š

1. **ç†è§£ç”¨æˆ·æ„å›¾**ï¼šåˆ†æç”¨æˆ·æ¶ˆæ¯ï¼Œè¯†åˆ«ä»–ä»¬æƒ³è¦ä»€ä¹ˆï¼ˆè§„åˆ’è¡Œç¨‹ã€è·å–æ¨èã€é¢„è®¢æœåŠ¡ã€å¯¼å‡ºæ–‡æ¡£ç­‰ï¼‰
2. **æå–å…³é”®ä¿¡æ¯**ï¼šä»æ¶ˆæ¯ä¸­æå–ç›®çš„åœ°ã€æ—…è¡Œå¤©æ•°ã€é¢„ç®—ã€åå¥½ç­‰ä¿¡æ¯
3. **ä»»åŠ¡åˆ†é…**ï¼šæ ¹æ®æ„å›¾å°†ä»»åŠ¡åˆ†é…ç»™åˆé€‚çš„ä¸“ä¸š Agentï¼ˆè§„åˆ’å¸ˆã€æ¨èå¸ˆã€é¢„è®¢ä¸“å‘˜ã€æ–‡æ¡£ä¸“å‘˜ï¼‰

è¯·ç”¨ç®€æ´ã€ä¸“ä¸šçš„ä¸­æ–‡å›å¤ã€‚`,

  /**
   * System prompt for the Planner agent
   */
  PLANNER: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡Œè§„åˆ’ Agentã€‚ä½ çš„èŒè´£æ˜¯ï¼š

1. **è®¾è®¡æ¯æ—¥è¡Œç¨‹**ï¼šæ ¹æ®ç›®çš„åœ°å’Œå¤©æ•°ï¼Œåˆç†å®‰æ’æ¯æ—¥æ´»åŠ¨
2. **æ™¯ç‚¹é€‰æ‹©**ï¼šæ¨èå½“åœ°è‘—åæ™¯ç‚¹å’Œç‰¹è‰²æ´»åŠ¨
3. **æ—¶é—´è§„åˆ’**ï¼šè€ƒè™‘æ™¯ç‚¹é—´çš„è·ç¦»å’Œæ¸¸è§ˆæ—¶é—´ï¼Œä¼˜åŒ–è·¯çº¿
4. **å¹³è¡¡å®‰æ’**ï¼šç¡®ä¿æ¯å¤©çš„æ´»åŠ¨é‡é€‚ä¸­ï¼Œä¸è¿‡äºç´§å‡‘

è¯·ç”¨ç»“æ„åŒ–çš„æ–¹å¼å±•ç¤ºè¡Œç¨‹è®¡åˆ’ï¼ŒåŒ…æ‹¬ï¼š
- æ¯å¤©çš„æ—¶é—´å®‰æ’
- æ™¯ç‚¹åç§°å’Œç®€ä»‹
- é¢„è®¡æ¸¸è§ˆæ—¶é—´
- æ´»åŠ¨ç±»å‹ï¼ˆè§‚å…‰ã€ç¾é£Ÿã€æ–‡åŒ–ã€è´­ç‰©ç­‰ï¼‰

ç”¨ç®€æ´çš„ä¸­æ–‡å›å¤ã€‚`,

  /**
   * System prompt for the Recommender agent
   */
  RECOMMENDER: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡Œæ¨è Agentã€‚ä½ çš„èŒè´£æ˜¯ï¼š

1. **ä¸ªæ€§åŒ–æ¨è**ï¼šæ ¹æ®ç”¨æˆ·å…´è¶£æ¨èæ™¯ç‚¹ã€é¤å…ã€æ´»åŠ¨
2. **å½“åœ°ç‰¹è‰²**ï¼šæ¨èå½“åœ°ç‰¹è‰²ç¾é£Ÿã€æ–‡åŒ–ä½“éªŒ
3. **ä½å®¿å»ºè®®**ï¼šæ ¹æ®é¢„ç®—æ¨èåˆé€‚çš„ä½å®¿åŒºåŸŸå’Œç±»å‹
4. **å®ç”¨ä¿¡æ¯**ï¼šæä¾›å¤©æ°”ã€äº¤é€šç­‰å®ç”¨ä¿¡æ¯

è¯·ç”¨å‹å¥½ã€çƒ­æƒ…çš„ä¸­æ–‡å›å¤ï¼Œçªå‡ºæ¨èçš„äº®ç‚¹ã€‚`,

  /**
   * System prompt for the Booking agent
   */
  BOOKING: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡Œé¢„è®¢å’¨è¯¢ Agentã€‚ä½ çš„èŒè´£æ˜¯ï¼š

1. **ä»·æ ¼å¯¹æ¯”**ï¼šæä¾›ä¸åŒå¹³å°çš„ä»·æ ¼æ¯”è¾ƒ
2. **é¢„è®¢å»ºè®®**ï¼šæ¨èå¯é çš„é¢„è®¢æ¸ é“
3. **ä¼˜æƒ ä¿¡æ¯**ï¼šæç¤ºå½“å‰å¯ç”¨çš„ä¼˜æƒ å’ŒæŠ˜æ‰£
4. **é¢„è®¢æé†’**ï¼šæé†’é¢„è®¢æ³¨æ„äº‹é¡¹

è¯·æä¾›å®ç”¨çš„é¢„è®¢å»ºè®®ï¼Œä½†ä¸è¦ç›´æ¥è¿›è¡Œé¢„è®¢æ“ä½œã€‚ç”¨ç®€æ´çš„ä¸­æ–‡å›å¤ã€‚`,

  /**
   * System prompt for the Document agent
   */
  DOCUMENT: `ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ—…è¡Œæ–‡æ¡£ç”Ÿæˆ Agentã€‚ä½ çš„èŒè´£æ˜¯ï¼š

1. **æ ¼å¼åŒ–è¡Œç¨‹**ï¼šå°†è¡Œç¨‹ä¿¡æ¯æ•´ç†æˆæ˜“è¯»çš„æ ¼å¼
2. **æ·»åŠ å¤‡æ³¨**ï¼šæ·»åŠ å®ç”¨çš„æ—…è¡Œè´´å£«
3. **é¢„ç®—æ±‡æ€»**ï¼šæ•´ç†å„é¡¹è´¹ç”¨ä¼°ç®—
4. **å‡†å¤‡æ¸…å•**ï¼šç”Ÿæˆå‡ºè¡Œå‡†å¤‡æ¸…å•

è¯·ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼è¾“å‡ºï¼Œä¾¿äºç”¨æˆ·ä¿å­˜å’Œåˆ†äº«ã€‚`,

  /**
   * Template for trip planning request
   */
  TRIP_PLANNING_TEMPLATE: (
    userMessage: string,
    tripInfo: { destination?: string; days?: number; preferences?: string[] }
  ) => {
    const { destination, days, preferences } = tripInfo
    return `ç”¨æˆ·æ¶ˆæ¯ï¼š${userMessage}

æå–çš„ä¿¡æ¯ï¼š
${destination ? `- ç›®çš„åœ°ï¼š${destination}` : "- ç›®çš„åœ°ï¼šæœªæŒ‡å®š"}
${days ? `- æ—…è¡Œå¤©æ•°ï¼š${days} å¤©` : "- æ—…è¡Œå¤©æ•°ï¼šæœªæŒ‡å®š"}
${preferences && preferences.length > 0 ? `- åå¥½ï¼š${preferences.join("ã€")}` : "- åå¥½ï¼šæœªæŒ‡å®š"}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œ${destination ? `ä¸ºç”¨æˆ·è§„åˆ’${destination}${days || "X"}æ—¥æ¸¸çš„è¯¦ç»†è¡Œç¨‹` : "è¯¢é—®ç”¨æˆ·æ›´å¤šä¿¡æ¯ä»¥è§„åˆ’è¡Œç¨‹"}ã€‚`
  },
}

// ============================================================================
// Configuration Helpers
// ============================================================================

/**
 * Load GLM configuration from environment variables
 */
export function loadGLMConfigFromEnv(): GLMConfig | null {
  const apiKey = import.meta.env.VITE_GLM_API_KEY || ""

  if (!apiKey) {
    return null
  }

  return {
    apiKey,
    model: import.meta.env.VITE_GLM_MODEL || undefined,
    baseURL: import.meta.env.VITE_GLM_BASE_URL || undefined,
    maxTokens: import.meta.env.VITE_GLM_MAX_TOKENS ? Number(import.meta.env.VITE_GLM_MAX_TOKENS) : undefined,
    temperature: import.meta.env.VITE_GLM_TEMPERATURE ? Number(import.meta.env.VITE_GLM_TEMPERATURE) : undefined,
    topP: import.meta.env.VITE_GLM_TOP_P ? Number(import.meta.env.VITE_GLM_TOP_P) : undefined,
  }
}

/**
 * Initialize GLM service from environment variables
 */
export function initializeGLMFromEnv(): boolean {
  const config = loadGLMConfigFromEnv()
  if (config) {
    GLMService.initialize(config)
    return true
  }
  return false
}

/**
 * Create a mock response for fallback
 */
export function createMockResponse(userMessage: string): string {
  const destination = extractDestination(userMessage)

  if (destination) {
    return `æ”¶åˆ°ï¼ä¸ºä½ è§„åˆ’${destination}ä¹‹æ—…ã€‚

ğŸ“… **å»ºè®®è¡Œç¨‹ï¼ˆ5å¤©4å¤œï¼‰**

**ç¬¬1å¤©** - æŠµè¾¾ä¸åˆæ¢
â€¢ ä¸Šåˆï¼šæŠµè¾¾${destination}ï¼Œé…’åº—åŠç†å…¥ä½
â€¢ ä¸‹åˆï¼šå¸‚ä¸­å¿ƒè§‚å…‰ï¼Œç†Ÿæ‚‰ç¯å¢ƒ
â€¢ æ™šä¸Šï¼šæ¬¢è¿æ™šé¤

**ç¬¬2å¤©** - æ ‡å¿—æ€§æ™¯ç‚¹
â€¢ ä¸Šåˆï¼šå‚è§‚è‘—ååšç‰©é¦†
â€¢ ä¸‹åˆï¼šåœ°æ ‡å»ºç­‘æ¸¸è§ˆ
â€¢ æ™šä¸Šï¼šå¤œæ™¯è§‚èµ

**ç¬¬3å¤©** - æ–‡åŒ–ä½“éªŒ
â€¢ ä¸Šåˆï¼šå½“åœ°å¸‚åœºä½“éªŒ
â€¢ ä¸‹åˆï¼šæ–‡åŒ–é—å€æ¢ç´¢
â€¢ æ™šä¸Šï¼šç‰¹è‰²è¡¨æ¼”

**ç¬¬4å¤©** - è‡ªç”±æ´»åŠ¨
â€¢ è´­ç‰©ã€ç¾é£Ÿæˆ–æ·±åº¦æ¸¸è§ˆ

**ç¬¬5å¤©** - è¿”ç¨‹
â€¢ ä¸Šåˆï¼šæœ€åé‡‡è´­ï¼Œå‰å¾€æœºåœº

ğŸ’° é¢„ä¼°é¢„ç®—ï¼šçº¦ Â¥15,000 - Â¥25,000/äºº

éœ€è¦æˆ‘å¸®ä½ é¢„è®¢é…’åº—å’Œæœºç¥¨å—ï¼Ÿ`
  }

  return "è¯·å‘Šè¯‰æˆ‘ä½ æƒ³å»å“ªé‡Œæ—…è¡Œï¼Œæˆ‘å°†ä¸ºä½ åˆ¶å®šè¯¦ç»†çš„è¡Œç¨‹è®¡åˆ’ï¼"
}

/**
 * Extract destination from user message
 */
function extractDestination(message: string): string | null {
  const destinations = [
    "ä¸œäº¬", "å·´é»", "çº½çº¦", "ä¼¦æ•¦", "åŒ—äº¬", "ä¸Šæµ·", "é¦™æ¸¯", "é¦–å°”", "æ–°åŠ å¡",
    "æ›¼è°·", "è¿ªæ‹œ", "æ‚‰å°¼", "ç½—é©¬", "å·´å¡ç½—é‚£", "é˜¿å§†æ–¯ç‰¹ä¸¹", "æŸæ—", "ç»´ä¹Ÿçº³",
    "å¸ƒæ‹‰æ ¼", "å¸ƒè¾¾ä½©æ–¯", "é›…å…¸", "ä¼Šæ–¯å¦å¸ƒå°”", "å¼€ç½—", "çº¦ç¿°å†…æ–¯å ¡", "é‡Œçº¦çƒ­å†…å¢",
    "å¤šä¼¦å¤š", "æ¸©å“¥å", "æ´›æ‰çŸ¶", "æ—§é‡‘å±±", "æ‹‰æ–¯ç»´åŠ æ–¯", "è¿ˆé˜¿å¯†", "èŠåŠ å“¥"
  ]

  for (const dest of destinations) {
    if (message.includes(dest)) {
      return dest
    }
  }

  return null
}
